{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for batch processing news sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/tickerList.pckl', 'rb')\n",
    "tickerList= dill.load(f)\n",
    "f.close()\n",
    "\n",
    "news=pd.read_csv('./data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        super().__init__() #Initialize the model by setting up the layers\n",
    "        self.vocab_size = vocab_size #vocab_size : The vocabulary size.\n",
    "        self.embed_size = embed_size #embed_size : The embedding layer size.\n",
    "        self.lstm_size = lstm_size #lstm_size : The LSTM layer size\n",
    "        self.output_size = output_size # output_size : The output size\n",
    "        self.lstm_layers = lstm_layers # lstm_layers : The number of LSTM layers\n",
    "        self.dropout = dropout #dropout : The dropout probability\n",
    "        \n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, \n",
    "                            dropout=dropout, batch_first=False)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        self.lsof = nn.LogSoftmax(dim=None)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size): #Initializes hidden state #Parameter batch_size: The size of batches\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden  # returns hidden_state  \n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state): # Perform a forward pass of our model on nn_input.\n",
    "     #PARAMETERS nn_input: The batch of input to the NN. hidden_state: The LSTM hidden state.\n",
    "    \n",
    "        # embeddings and lstm_out\n",
    "        nn_input = nn_input.long()\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden_state)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax function\n",
    "        logps = self.lsof(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        batch_size = nn_input.size(1)       \n",
    "        \n",
    "        # return last sigmoid output (log softmax output) and the new hidden state\n",
    "        return logps, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./data/model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/twits.json', 'r') as f: twits = json.load(f)\n",
    "# with open('test_data.json', 'r') as f: test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(r'[$][A-Za-z][\\S]*', ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(r'[@][A-Za-z][\\S]*', ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [preprocess(twit) for twit in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary by using Bag of words\n",
    "all_text = ' '.join([' '.join(tokens) for tokens in tokenized])\n",
    "counts = Counter(all_text.split())\n",
    "bow = dict(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(bow)\n",
    "freqs = {word: count/total_count for word, count in bow.items()}\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 1e-5\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 10\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = sorted(freqs.items(), key=operator.itemgetter(1))[-high_cutoff:]\n",
    "K_most_common = [x[0] for x in K_most_common]\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab ={word: index for index, word in enumerate(filtered_words, 1)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii: word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "filtered = [[word for word in message if word in vocab] for message in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab): # Make a prediction on a single sentence.\n",
    "\n",
    "    tokens = preprocess(text)\n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    # Convert words to ids\n",
    "    tokens =  [vocab[token] for token in tokens]\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.FloatTensor(tokens).view(-1, 1)))\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "    logps, _ = model.forward(text_input, hidden)\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps)\n",
    "\n",
    "    return pred #pred : Prediction vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\tf_kernel\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7.6556e-05, 1.3917e-02, 6.3105e-03, 6.7737e-01, 3.0233e-01]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['score'] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]C:\\tools\\Anaconda3\\envs\\tf_kernel\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  2%|▏         | 1048/50000 [13:45<9:58:16,  1.36it/s] "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1050000, 1100000)):\n",
    "    text = str(news.loc[i, ['headline']]) + \" \" +str(news.loc[i, ['message_body']])\n",
    "    sentimentscore = np.dot(predict(text, model, vocab).detach().numpy(),np.linspace(-2,2,5))\n",
    "    news['score'].iloc[i] = sentimentscore\n",
    "\n",
    "news.to_csv('./data/news1050000to1100000.csv', header=True)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('./data/news1050000to1100000.csv', header=True)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_kernel]",
   "language": "python",
   "name": "conda-env-tf_kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
