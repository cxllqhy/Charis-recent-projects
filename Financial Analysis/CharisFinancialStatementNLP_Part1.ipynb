{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ej1MdAyQKz1K"
   },
   "source": [
    "# NLP on Financial Statements Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "colab_type": "code",
    "id": "Ms4s6MUAKz1L",
    "outputId": "d8bf4537-7c38-48a2-9b99-0d018887531d"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install cloudpickle alphalens nltk numpy ratelimit requests scikit-learn six tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install ratelimit\n",
    "# conda install -c cvxgrp cvxpy\n",
    "# conda install -c conda-forge cycler\n",
    "# conda install -c plotly plotly\n",
    "# conda install -c anaconda pyparsing\n",
    "# conda install -c anaconda pytz\n",
    "# conda install scikit-learn #python 3.7.0 and rolls back conda install\n",
    "# pip install zipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40eCSY1CKz1N"
   },
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edC3UrRvKz1O"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import ratelimit\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PD__ksoWKz1Q"
   },
   "source": [
    "### Download NLP Corpora\n",
    "The stopwords corpus for removing stopwords and wordnet for lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "TI5hmbm2Kz1Q",
    "outputId": "f7add76a-98e2-4160-a7f4-6d48b30a3d5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaEvNtzgL3hn"
   },
   "source": [
    "## Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovZC8Cj0L6mF"
   },
   "outputs": [],
   "source": [
    "class SecAPI(object):\n",
    "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
    "    @staticmethod\n",
    "    @sleep_and_retry\n",
    "    # Dividing the call limit by half to avoid coming close to the limit\n",
    "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
    "    def _call_sec(url):\n",
    "        return requests.get(url)\n",
    "\n",
    "    def get(self, url):\n",
    "        return self._call_sec(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovZC8Cj0L6mF"
   },
   "outputs": [],
   "source": [
    "def print_twenty_F_data(twenty_F_data, fields, field_length_limit=50):\n",
    "    indentation = '  '\n",
    "\n",
    "    print('[')\n",
    "    for twenty_F in twenty_F_data:\n",
    "        print_statement = '{}{{'.format(indentation)\n",
    "        for field in fields:\n",
    "            value = str(twenty_F[field])\n",
    "\n",
    "            # Show return lines in output\n",
    "            if isinstance(value, str):\n",
    "                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n",
    "            else:\n",
    "                value_str = str(value)\n",
    "\n",
    "            # Cut off the string if it gets too long\n",
    "            if len(value_str) > field_length_limit:\n",
    "                value_str = value_str[:field_length_limit] + '...'\n",
    "\n",
    "            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n",
    "\n",
    "        print_statement += '},'\n",
    "        print(print_statement)\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovZC8Cj0L6mF"
   },
   "outputs": [],
   "source": [
    "def plot_similarities(similarities_list, dates, title, labels):\n",
    "    assert len(similarities_list) == len(labels)\n",
    "\n",
    "    plt.figure(1, figsize=(10, 7))\n",
    "    for similarities, label in zip(similarities_list, labels):\n",
    "        plt.title(title)\n",
    "        plt.plot(dates, similarities, label=label)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgCoyuxvKz1S"
   },
   "source": [
    "## Get 20-Fs\n",
    "We'll be running NLP analysis on 20-F documents. To do that, we first need to download the documents. For this project, we'll download 20-Fs for a few companies. To lookup documents for these companies, we'll use their CIK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "th1l9_8-cA9U"
   },
   "outputs": [],
   "source": [
    "tickerList_df = pd.read_csv('OTC.csv')\n",
    "tickerList = tickerList_df['Ticker'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDjFGX5dL5ow"
   },
   "outputs": [],
   "source": [
    "def getCIKs(TICKERS):\n",
    "    URL = 'http://www.sec.gov/cgi-bin/browse-edgar?CIK={}&Find=Search&owner=exclude&action=getcompany'\n",
    "    CIK_RE = re.compile(r'.*CIK=(\\d{10}).*')\n",
    "    cik_dict = {}\n",
    "    for ticker in TICKERS:\n",
    "        f = requests.get(URL.format(ticker), stream = True)\n",
    "        results = CIK_RE.findall(f.text)\n",
    "        if len(results):\n",
    "            results[0] = int(re.sub('\\.[0]*', '.', results[0]))\n",
    "            cik_dict[str(ticker).upper()] = str(results[0])\n",
    "    f = open('cik_dict', 'w')   \n",
    "    print(cik_dict)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEIMcfxILvg6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ZXAIY': '1506756', 'LDKYQ': '1385424', 'LASLY': '1499673', 'LGFTY': '1412494', 'YZCAY': '1048098', 'ZAHLY': '1501176'}\n"
     ]
    }
   ],
   "source": [
    "with open('OTC_tickerList.txt', 'w') as f:\n",
    "    print(getCIKs(tickerList), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vctuHmqXcSsL"
   },
   "source": [
    "### OTC stocks with CIK numbers\n",
    "{'ZXAIY': '1506756', 'LDKYQ': '1385424', 'LASLY': '1499673', 'LGFTY': '1412494', 'YZCAY': '1048098', 'ZAHLY': '1501176'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the same method to obtain the CIK for NYSE and NASDAQ stocks:\n",
    "\n",
    "{'YI': '1738906', 'VNET': '1508475', 'QFIN': '1741530', 'WBAI': '1517496', 'JOBS': '1295484', 'WUBA': '1525494', 'ATV': '1365742', 'AMCN': '1413745', 'BABA': '1577552', 'ACH': '1161611', 'ATAI': '1420529', 'JG': '1737339', 'ATHM': '1527636', 'BIDU': '1329099', 'BZUN': '1625414', 'BILI': '1723690', 'BEDU': '1696355', 'CANG': '1725123', 'CYOU': '1458696', 'CMCM': '1597835', 'DL': '1438644', 'CEA': '1030475', 'JRJC': '1297830', 'LFC': '1268896', 'HTHT': '1483994', 'CHL': '1117795', 'CEO': '1095595', 'BORN': '1490366', 'COE': '1659494', 'SNP': '1123658', 'XRF': '1346610', 'ZNH': '1041668', 'CNTF': '1316317', 'CHA': '1191255', 'CHU': '1113866', 'CCIH': '1498576', 'CCM': '1472072', 'CTRP': '1269238', 'DQ': '1477641', 'EHIC': '1517492', 'SFUN': '1294404', 'FANH': '1413855', 'GDS': '1526125', 'GHG': '1724755', 'GSUM': '1647338', 'GSH': '1012139', 'HLG': '1596964', 'HQCL': '1371541', 'HX': '1702318', 'HMI': '1720446', 'HNP': '929058', 'HCM': '1648257', 'HUYA': '1728190', 'KANG': '1524190', 'IQ': '1722608', 'JD': '1549802', 'JT': '1713923', 'JKS': '1481513', 'JMU': '1527762', 'JMEI': '1597680', 'JP': '1616291', 'KZ': '1285137', 'LEJU': '1596856', 'LX': '1708259', 'LITB': '1523836', 'MOMO': '1610601', 'NTES': '1110646', 'EDU': '1372920', 'NIO': '1736541', 'NOAH': '1499543', 'ONE': '1722380', 'OSN': '1485538', 'PTR': '1108329', 'FENG': '1509646', 'PDD': '1737806', 'PPDF': '1691445', 'QD': '1692705', 'SOL': '1417892', 'RENN': '1509223', 'REDU': '1712178', 'RYB': '1708441', 'SECO': '1633441', 'SHI': '908732', 'SKYS': '1594124', 'SOGO': '1713947', 'SOHU': '1734107', 'TAL': '1499620', 'TEDU': '1592560', 'TME': '1744676', 'NCTY': '1296774', 'TC': '1743340', 'TOUR': '1597095', 'VIPS': '1529192', 'WB': '1595761', 'XYF': '1725033', 'XIN': '1398453', 'XNET': '1510593', 'YIN': '1661125', 'YRD': '1631761', 'YY': '1530238', 'ZLAB': '1704292', 'ZPIN': '1378564', 'ZTO': '1677250'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {'YI': '1738906', 'VNET': '1508475', 'QFIN': '1741530', 'WBAI': '1517496', 'JOBS': '1295484', 'WUBA': '1525494', 'ATV': '1365742', 'AMCN': '1413745', 'BABA': '1577552', 'ACH': '1161611', 'ATAI': '1420529', 'JG': '1737339', 'ATHM': '1527636', 'BIDU': '1329099', 'BZUN': '1625414', 'BILI': '1723690', 'BEDU': '1696355', 'CANG': '1725123', 'CYOU': '1458696', 'CMCM': '1597835', 'DL': '1438644', 'CEA': '1030475', 'JRJC': '1297830', 'LFC': '1268896', 'HTHT': '1483994', 'CHL': '1117795', 'CEO': '1095595', 'BORN': '1490366', 'COE': '1659494', 'SNP': '1123658', 'XRF': '1346610', 'ZNH': '1041668', 'CNTF': '1316317', 'CHA': '1191255', 'CHU': '1113866', 'CCIH': '1498576', 'CCM': '1472072', 'CTRP': '1269238', 'DQ': '1477641', 'EHIC': '1517492', 'SFUN': '1294404', 'FANH': '1413855', 'GDS': '1526125', 'GHG': '1724755', 'GSUM': '1647338', 'GSH': '1012139', 'HLG': '1596964', 'HQCL': '1371541', 'HX': '1702318', 'HMI': '1720446', 'HNP': '929058', 'HCM': '1648257', 'HUYA': '1728190', 'KANG': '1524190', 'IQ': '1722608', 'JD': '1549802', 'JT': '1713923', 'JKS': '1481513', 'JMU': '1527762', 'JMEI': '1597680', 'JP': '1616291', 'KZ': '1285137', 'LEJU': '1596856', 'LX': '1708259', 'LITB': '1523836', 'MOMO': '1610601', 'NTES': '1110646', 'EDU': '1372920', 'NIO': '1736541', 'NOAH': '1499543', 'ONE': '1722380', 'OSN': '1485538', 'PTR': '1108329', 'FENG': '1509646', 'PDD': '1737806', 'PPDF': '1691445', 'QD': '1692705', 'SOL': '1417892', 'RENN': '1509223', 'REDU': '1712178', 'RYB': '1708441', 'SECO': '1633441', 'SHI': '908732', 'SKYS': '1594124', 'SOGO': '1713947', 'SOHU': '1734107', 'TAL': '1499620', 'TEDU': '1592560', 'TME': '1744676', 'NCTY': '1296774', 'TC': '1743340', 'TOUR': '1597095', 'VIPS': '1529192', 'WB': '1595761', 'XYF': '1725033', 'XIN': '1398453', 'XNET': '1510593', 'YIN': '1661125', 'YRD': '1631761', 'YY': '1530238', 'ZLAB': '1704292', 'ZPIN': '1378564', 'ZTO': '1677250', 'ZXAIY': '1506756', 'LDKYQ': '1385424', 'LASLY': '1499673', 'LGFTY': '1412494', 'YZCAY': '1048098', 'ZAHLY': '1501176'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_tickerList_dictionary.txt', 'w') as f:\n",
    "    print(cik_lookup, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdePRGx5Kz1U"
   },
   "source": [
    "### Get list of 20-Fs\n",
    "The SEC has a limit on the number of calls you can make to the website per second. In order to avoid hiding that limit, we've created the `SecAPI` class. This will cache data from the SEC and prevent you from going over the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHTbEt08Kz1U"
   },
   "outputs": [],
   "source": [
    "sec_api = SecAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjhsccviKz1W"
   },
   "source": [
    "With the class constructed, let's pull a list of filled 20-Fs from the SEC for each company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3Vu_qUYBOGw"
   },
   "source": [
    "It is important to know the particular company's fiscal year in order to distinguish financial quarterly related 6-Ks from other informational disclosure 6-Ks. At the end of the fiscal year and last quarter, Chinese ADRs file a 20-F report, which provides the annual statements for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ys4nAi0Kz1X"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    newest_pricing_data = pd.to_datetime('2019-05-13')\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)\n",
    "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzA5_nYRKz1Y"
   },
   "source": [
    "Let's pull the list using the `get_sec_data` function, then display some of the results. For displaying some of the data, we'll use ZXAIY as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbAo6Kd8Kz1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/1506756/000119312519120866/0001193125-19-120866-index.htm',\n",
      "  '20-F',\n",
      "  '2019-04-26'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1506756/000119312518136578/0001193125-18-136578-index.htm',\n",
      "  '20-F',\n",
      "  '2018-04-27'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1506756/000119312517145094/0001193125-17-145094-index.htm',\n",
      "  '20-F',\n",
      "  '2017-04-28'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1506756/000119312516560893/0001193125-16-560893-index.htm',\n",
      "  '20-F',\n",
      "  '2016-04-28'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1506756/000119312515161773/0001193125-15-161773-index.htm',\n",
      "  '20-F',\n",
      "  '2015-04-30')]\n"
     ]
    }
   ],
   "source": [
    "example_ticker = 'ZXAIY'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '20-F')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kUTq6XEKz1b"
   },
   "source": [
    "### Download 20-Fs\n",
    "As you see, this is a list of urls. These urls point to a file that contains metadata related to each filling. Since we don't care about the metadata, we'll pull the filling by replacing the url with the filling url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qn4GcnsjKz1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading YI Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.07filling/s]\n",
      "Downloading VNET Fillings: 100%|██████████| 9/9 [00:04<00:00,  2.05filling/s]\n",
      "Downloading QFIN Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.02filling/s]\n",
      "Downloading WBAI Fillings: 100%|██████████| 7/7 [00:03<00:00,  1.98filling/s]\n",
      "Downloading JOBS Fillings: 100%|██████████| 16/16 [00:07<00:00,  2.32filling/s]\n",
      "Downloading WUBA Fillings: 100%|██████████| 9/9 [00:03<00:00,  2.24filling/s]\n",
      "Downloading ATV Fillings: 100%|██████████| 16/16 [00:05<00:00,  2.50filling/s]\n",
      "Downloading AMCN Fillings: 100%|██████████| 14/14 [00:05<00:00,  2.43filling/s]\n",
      "Downloading BABA Fillings: 100%|██████████| 4/4 [00:02<00:00,  1.58filling/s]\n",
      "Downloading ACH Fillings: 100%|██████████| 28/28 [00:12<00:00,  2.34filling/s]\n",
      "Downloading ATAI Fillings: 100%|██████████| 13/13 [00:06<00:00,  2.17filling/s]\n",
      "Downloading JG Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.84filling/s]\n",
      "Downloading ATHM Fillings: 100%|██████████| 6/6 [00:03<00:00,  1.89filling/s]\n",
      "Downloading BIDU Fillings: 100%|██████████| 16/16 [00:06<00:00,  2.45filling/s]\n",
      "Downloading BZUN Fillings: 100%|██████████| 5/5 [00:02<00:00,  2.27filling/s]\n",
      "Downloading BILI Fillings: 100%|██████████| 2/2 [00:00<00:00,  4.25filling/s]\n",
      "Downloading BEDU Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.87filling/s]\n",
      "Downloading CANG Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.77filling/s]\n",
      "Downloading CYOU Fillings: 100%|██████████| 10/10 [00:05<00:00,  1.73filling/s]\n",
      "Downloading CMCM Fillings: 100%|██████████| 6/6 [00:02<00:00,  2.12filling/s]\n",
      "Downloading DL Fillings: 100%|██████████| 13/13 [00:06<00:00,  2.20filling/s]\n",
      "Downloading CEA Fillings: 100%|██████████| 21/21 [00:08<00:00,  2.60filling/s]\n",
      "Downloading JRJC Fillings: 100%|██████████| 20/20 [00:07<00:00,  2.44filling/s]\n",
      "Downloading LFC Fillings: 100%|██████████| 16/16 [00:07<00:00,  2.25filling/s]\n",
      "Downloading HTHT Fillings: 100%|██████████| 9/9 [00:04<00:00,  2.06filling/s]\n",
      "Downloading CHL Fillings: 100%|██████████| 19/19 [00:09<00:00,  2.23filling/s]\n",
      "Downloading CEO Fillings: 100%|██████████| 21/21 [00:10<00:00,  2.26filling/s]\n",
      "Downloading BORN Fillings: 100%|██████████| 9/9 [00:05<00:00,  2.00filling/s]\n",
      "Downloading COE Fillings: 100%|██████████| 3/3 [00:01<00:00,  1.65filling/s]\n",
      "Downloading SNP Fillings: 100%|██████████| 19/19 [00:08<00:00,  2.42filling/s]\n",
      "Downloading XRF Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.91filling/s]\n",
      "Downloading ZNH Fillings: 100%|██████████| 20/20 [00:12<00:00,  2.33filling/s]\n",
      "Downloading CNTF Fillings: 100%|██████████| 15/15 [00:06<00:00,  2.33filling/s]\n",
      "Downloading CHA Fillings: 100%|██████████| 17/17 [00:10<00:00,  1.22filling/s]\n",
      "Downloading CHU Fillings: 100%|██████████| 19/19 [00:09<00:00,  2.03filling/s]\n",
      "Downloading CCIH Fillings: 100%|██████████| 9/9 [00:04<00:00,  1.91filling/s]\n",
      "Downloading CCM Fillings: 100%|██████████| 12/12 [00:05<00:00,  2.41filling/s]\n",
      "Downloading CTRP Fillings: 100%|██████████| 19/19 [00:07<00:00,  2.24filling/s]\n",
      "Downloading DQ Fillings: 100%|██████████| 11/11 [00:04<00:00,  2.37filling/s]\n",
      "Downloading EHIC Fillings: 100%|██████████| 5/5 [00:02<00:00,  2.10filling/s]\n",
      "Downloading SFUN Fillings: 100%|██████████| 14/14 [00:04<00:00,  2.56filling/s]\n",
      "Downloading FANH Fillings: 100%|██████████| 15/15 [00:06<00:00,  2.21filling/s]\n",
      "Downloading GDS Fillings: 100%|██████████| 3/3 [00:01<00:00,  1.62filling/s]\n",
      "Downloading GHG Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.85filling/s]\n",
      "Downloading GSUM Fillings: 100%|██████████| 4/4 [00:01<00:00,  2.02filling/s]\n",
      "Downloading GSH Fillings: 100%|██████████| 18/18 [00:08<00:00,  2.22filling/s]\n",
      "Downloading HLG Fillings: 100%|██████████| 5/5 [00:02<00:00,  1.93filling/s]\n",
      "Downloading HQCL Fillings: 100%|██████████| 14/14 [00:06<00:00,  2.57filling/s]\n",
      "Downloading HX Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.80filling/s]\n",
      "Downloading HMI Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.17filling/s]\n",
      "Downloading HNP Fillings: 100%|██████████| 19/19 [00:08<00:00,  2.42filling/s]\n",
      "Downloading HCM Fillings: 100%|██████████| 4/4 [00:01<00:00,  2.00filling/s]\n",
      "Downloading HUYA Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.91filling/s]\n",
      "Downloading KANG Fillings: 100%|██████████| 7/7 [00:02<00:00,  2.20filling/s]\n",
      "Downloading IQ Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.72filling/s]\n",
      "Downloading JD Fillings: 100%|██████████| 6/6 [00:02<00:00,  1.79filling/s]\n",
      "Downloading JT Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.93filling/s]\n",
      "Downloading JKS Fillings: 100%|██████████| 13/13 [00:04<00:00,  3.11filling/s]\n",
      "Downloading JMU Fillings: 100%|██████████| 6/6 [00:01<00:00,  2.60filling/s]\n",
      "Downloading JMEI Fillings: 100%|██████████| 5/5 [00:03<00:00,  1.51filling/s]\n",
      "Downloading JP Fillings: 100%|██████████| 4/4 [00:02<00:00,  1.75filling/s]\n",
      "Downloading KZ Fillings: 100%|██████████| 13/13 [00:06<00:00,  2.18filling/s]\n",
      "Downloading LEJU Fillings: 100%|██████████| 5/5 [00:02<00:00,  1.82filling/s]\n",
      "Downloading LX Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.62filling/s]\n",
      "Downloading LITB Fillings: 100%|██████████| 6/6 [00:04<00:00,  1.57filling/s]\n",
      "Downloading MOMO Fillings: 100%|██████████| 6/6 [00:02<00:00,  2.17filling/s]\n",
      "Downloading NTES Fillings: 100%|██████████| 21/21 [00:09<00:00,  2.84filling/s]\n",
      "Downloading EDU Fillings: 100%|██████████| 15/15 [00:07<00:00,  2.46filling/s]\n",
      "Downloading NIO Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.75filling/s]\n",
      "Downloading NOAH Fillings: 100%|██████████| 10/10 [00:04<00:00,  2.22filling/s]\n",
      "Downloading ONE Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.58filling/s]\n",
      "Downloading OSN Fillings: 100%|██████████| 14/14 [00:04<00:00,  2.42filling/s]\n",
      "Downloading PTR Fillings: 100%|██████████| 19/19 [00:08<00:00,  2.43filling/s]\n",
      "Downloading FENG Fillings: 100%|██████████| 9/9 [00:04<00:00,  2.25filling/s]\n",
      "Downloading PDD Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.90filling/s]\n",
      "Downloading PPDF Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.87filling/s]\n",
      "Downloading QD Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.51filling/s]\n",
      "Downloading SOL Fillings: 100%|██████████| 15/15 [00:06<00:00,  2.53filling/s]\n",
      "Downloading RENN Fillings: 100%|██████████| 8/8 [00:04<00:00,  1.81filling/s]\n",
      "Downloading REDU Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.96filling/s]\n",
      "Downloading RYB Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.95filling/s]\n",
      "Downloading SECO Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.89filling/s]\n",
      "Downloading SHI Fillings: 100%|██████████| 22/22 [00:08<00:00,  2.36filling/s]\n",
      "Downloading SKYS Fillings: 100%|██████████| 4/4 [00:02<00:00,  1.71filling/s]\n",
      "Downloading SOGO Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.78filling/s]\n",
      "Downloading SOHU Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.71filling/s]\n",
      "Downloading TAL Fillings: 100%|██████████| 10/10 [00:04<00:00,  2.12filling/s]\n",
      "Downloading TEDU Fillings: 100%|██████████| 4/4 [00:02<00:00,  1.86filling/s]\n",
      "Downloading TME Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.61filling/s]\n",
      "Downloading NCTY Fillings: 100%|██████████| 16/16 [00:08<00:00,  2.53filling/s]\n",
      "Downloading TC Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.85filling/s]\n",
      "Downloading TOUR Fillings: 100%|██████████| 5/5 [00:02<00:00,  1.66filling/s]\n",
      "Downloading VIPS Fillings: 100%|██████████| 7/7 [00:03<00:00,  1.82filling/s]\n",
      "Downloading WB Fillings: 100%|██████████| 5/5 [00:02<00:00,  1.82filling/s]\n",
      "Downloading XYF Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.54filling/s]\n",
      "Downloading XIN Fillings: 100%|██████████| 16/16 [00:06<00:00,  2.59filling/s]\n",
      "Downloading XNET Fillings: 100%|██████████| 6/6 [00:02<00:00,  2.01filling/s]\n",
      "Downloading YIN Fillings: 100%|██████████| 3/3 [00:01<00:00,  1.75filling/s]\n",
      "Downloading YRD Fillings: 100%|██████████| 4/4 [00:02<00:00,  1.84filling/s]\n",
      "Downloading YY Fillings: 100%|██████████| 8/8 [00:04<00:00,  1.85filling/s]\n",
      "Downloading ZLAB Fillings: 100%|██████████| 2/2 [00:01<00:00,  1.88filling/s]\n",
      "Downloading ZPIN Fillings: 100%|██████████| 3/3 [00:01<00:00,  1.93filling/s]\n",
      "Downloading ZTO Fillings: 100%|██████████| 3/3 [00:01<00:00,  1.61filling/s]\n",
      "Downloading ZXAIY Fillings: 100%|██████████| 8/8 [00:04<00:00,  2.05filling/s]\n",
      "Downloading LDKYQ Fillings: 100%|██████████| 11/11 [00:03<00:00,  2.28filling/s]\n",
      "Downloading LASLY Fillings: 100%|██████████| 7/7 [00:01<00:00,  2.50filling/s]\n",
      "Downloading LGFTY Fillings: 100%|██████████| 3/3 [00:01<00:00,  2.38filling/s]\n",
      "Downloading YZCAY Fillings: 100%|██████████| 22/22 [00:09<00:00,  1.98filling/s]\n",
      "Downloading ZAHLY Fillings: 100%|██████████| 5/5 [00:02<00:00,  2.25filling/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0001193125-19-120866.txt : 20190426\n",
      "<SEC-HEADER>0001193125-19-120866.hdr.sgml : 20190426\n",
      "<ACCEPTANCE-DATETIME>20190426072508\n",
      "ACCESSION NUMBER:\t\t0001193125-19-120866\n",
      "CONFORMED SUBMISSION TYPE:\t20-F\n",
      "PUBLIC DOCUMENT COUNT:\t\t157\n",
      "CONFORMED PERIOD OF REPORT:\t20181231\n",
      "FILED AS OF DATE:\t\t20190426\n",
      "DATE AS OF CHANGE:\t\t20190426\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tChina Zenix Auto International Ltd\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001506756\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tMOTOR VEHICLE PARTS & ACCESSORIES [3714]\n",
      "\t\tIRS NUMBER:\t\t\t\t000000000\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t20-F\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t001-35154\n",
      "\t\tFILM NUMBER:\t\t19769356\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1608, North Circle Road State Highway\n",
      "\t\tSTREET 2:\t\tZhangzhou\n",
      "\t\tCITY:\t\t\tFujian Province\n",
      "\t\tSTATE:\t\t\tF4\n",
      "\t\tZIP:\t\t\t363000\n",
      "\t\tBUSINESS PHONE:\t\t(86) 596-2600308\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1608, North Circle Road State Highway\n",
      "\t\tSTREET 2:\t\tZhangzhou\n",
      "\t\tCITY:\t\t\tFujian Province\n",
      "\t\tSTATE:\t\t\tF4\n",
      "\t\tZIP:\t\t\t3...\n"
     ]
    }
   ],
   "source": [
    "raw_fillings_by_ticker = {}\n",
    "\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '20-F'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fuMmEd-Kz1d"
   },
   "source": [
    "### Get Documents\n",
    "With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `</DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `</DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag in between.\n",
    "\n",
    "Implement `get_documents` to return a list of these documents from a filling. Make sure not to include the tag in the returned document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXip3Ea8Kz1e"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_documents(text):\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    extracted_docs = []\n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "        doc = text[doc_start_i:doc_end_i]\n",
    "        extracted_docs.append(doc)\n",
    "    return extracted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efFDai3bKz1g"
   },
   "source": [
    "With the `get_documents` function implemented, let's extract all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EK81dkyKz1g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from YI Fillings: 100%|██████████| 1/1 [00:00<00:00, 32.35filling/s]\n",
      "Getting Documents from VNET Fillings: 100%|██████████| 8/8 [00:00<00:00,  9.33filling/s]\n",
      "Getting Documents from QFIN Fillings: 100%|██████████| 1/1 [00:00<00:00, 27.82filling/s]\n",
      "Getting Documents from WBAI Fillings: 100%|██████████| 6/6 [00:00<00:00, 11.73filling/s]\n",
      "Getting Documents from JOBS Fillings: 100%|██████████| 15/15 [00:00<00:00, 18.43filling/s]\n",
      "Getting Documents from WUBA Fillings: 100%|██████████| 6/6 [00:00<00:00, 29.20filling/s]\n",
      "Getting Documents from ATV Fillings: 100%|██████████| 12/12 [00:00<00:00, 48.12filling/s]\n",
      "Getting Documents from AMCN Fillings: 100%|██████████| 12/12 [00:00<00:00, 63.66filling/s]\n",
      "Getting Documents from BABA Fillings: 100%|██████████| 4/4 [00:00<00:00, 22.79filling/s]\n",
      "Getting Documents from ACH Fillings: 100%|██████████| 20/20 [00:00<00:00, 25.98filling/s]\n",
      "Getting Documents from ATAI Fillings: 100%|██████████| 12/12 [00:00<00:00, 20.53filling/s]\n",
      "Getting Documents from JG Fillings: 100%|██████████| 1/1 [00:00<00:00, 35.81filling/s]\n",
      "Getting Documents from ATHM Fillings: 100%|██████████| 6/6 [00:00<00:00, 44.88filling/s]\n",
      "Getting Documents from BIDU Fillings: 100%|██████████| 14/14 [00:00<00:00, 29.19filling/s]\n",
      "Getting Documents from BZUN Fillings: 100%|██████████| 4/4 [00:00<00:00, 23.31filling/s]\n",
      "Getting Documents from BILI Fillings: 100%|██████████| 1/1 [00:00<00:00, 55.70filling/s]\n",
      "Getting Documents from BEDU Fillings: 100%|██████████| 2/2 [00:00<00:00, 22.79filling/s]\n",
      "Getting Documents from CANG Fillings: 100%|██████████| 1/1 [00:00<00:00, 33.42filling/s]\n",
      "Getting Documents from CYOU Fillings: 100%|██████████| 10/10 [00:01<00:00,  8.44filling/s]\n",
      "Getting Documents from CMCM Fillings: 100%|██████████| 5/5 [00:00<00:00, 12.50filling/s]\n",
      "Getting Documents from DL Fillings: 100%|██████████| 11/11 [00:00<00:00, 32.92filling/s]\n",
      "Getting Documents from CEA Fillings: 100%|██████████| 18/18 [00:00<00:00, 34.31filling/s]\n",
      "Getting Documents from JRJC Fillings: 100%|██████████| 15/15 [00:01<00:00, 13.27filling/s]\n",
      "Getting Documents from LFC Fillings: 100%|██████████| 16/16 [00:00<00:00, 28.49filling/s]\n",
      "Getting Documents from HTHT Fillings: 100%|██████████| 9/9 [00:00<00:00, 22.50filling/s]\n",
      "Getting Documents from CHL Fillings: 100%|██████████| 19/19 [00:00<00:00, 63.08filling/s]\n",
      "Getting Documents from CEO Fillings: 100%|██████████| 18/18 [00:00<00:00, 25.78filling/s]\n",
      "Getting Documents from BORN Fillings: 100%|██████████| 8/8 [00:01<00:00,  6.00filling/s]\n",
      "Getting Documents from COE Fillings: 100%|██████████| 3/3 [00:01<00:00,  2.74filling/s]\n",
      "Getting Documents from SNP Fillings: 100%|██████████| 18/18 [00:04<00:00,  4.33filling/s]\n",
      "Getting Documents from XRF Fillings: 100%|██████████| 1/1 [00:00<00:00, 23.30filling/s]\n",
      "Getting Documents from ZNH Fillings: 100%|██████████| 18/18 [00:02<00:00,  7.92filling/s]\n",
      "Getting Documents from CNTF Fillings: 100%|██████████| 13/13 [00:00<00:00, 16.11filling/s]\n",
      "Getting Documents from CHA Fillings: 100%|██████████| 17/17 [00:00<00:00, 51.97filling/s]\n",
      "Getting Documents from CHU Fillings: 100%|██████████| 18/18 [00:00<00:00, 31.17filling/s]\n",
      "Getting Documents from CCIH Fillings: 100%|██████████| 8/8 [00:00<00:00,  9.55filling/s]\n",
      "Getting Documents from CCM Fillings: 100%|██████████| 10/10 [00:00<00:00, 21.33filling/s]\n",
      "Getting Documents from CTRP Fillings: 100%|██████████| 16/16 [00:00<00:00, 19.64filling/s]\n",
      "Getting Documents from DQ Fillings: 100%|██████████| 9/9 [00:00<00:00, 11.58filling/s]\n",
      "Getting Documents from EHIC Fillings: 100%|██████████| 4/4 [00:00<00:00,  6.29filling/s]\n",
      "Getting Documents from SFUN Fillings: 100%|██████████| 8/8 [00:00<00:00, 17.14filling/s]\n",
      "Getting Documents from FANH Fillings: 100%|██████████| 12/12 [00:00<00:00, 15.17filling/s]\n",
      "Getting Documents from GDS Fillings: 100%|██████████| 3/3 [00:00<00:00,  8.78filling/s]\n",
      "Getting Documents from GHG Fillings: 100%|██████████| 1/1 [00:00<00:00,  9.37filling/s]\n",
      "Getting Documents from GSUM Fillings: 100%|██████████| 3/3 [00:00<00:00, 12.13filling/s]\n",
      "Getting Documents from GSH Fillings: 100%|██████████| 18/18 [00:00<00:00, 18.55filling/s]\n",
      "Getting Documents from HLG Fillings: 100%|██████████| 4/4 [00:00<00:00, 13.02filling/s]\n",
      "Getting Documents from HQCL Fillings: 100%|██████████| 12/12 [00:01<00:00, 11.55filling/s]\n",
      "Getting Documents from HX Fillings: 100%|██████████| 1/1 [00:00<00:00, 29.49filling/s]\n",
      "Getting Documents from HMI Fillings: 100%|██████████| 2/2 [00:00<00:00, 16.44filling/s]\n",
      "Getting Documents from HNP Fillings: 100%|██████████| 18/18 [00:00<00:00, 26.54filling/s]\n",
      "Getting Documents from HCM Fillings: 100%|██████████| 3/3 [00:00<00:00,  3.92filling/s]\n",
      "Getting Documents from HUYA Fillings: 100%|██████████| 1/1 [00:00<00:00,  7.54filling/s]\n",
      "Getting Documents from KANG Fillings: 100%|██████████| 5/5 [00:00<00:00,  7.92filling/s]\n",
      "Getting Documents from IQ Fillings: 100%|██████████| 1/1 [00:00<00:00, 11.02filling/s]\n",
      "Getting Documents from JD Fillings: 100%|██████████| 5/5 [00:00<00:00, 11.94filling/s]\n",
      "Getting Documents from JT Fillings: 100%|██████████| 2/2 [00:00<00:00, 20.88filling/s]\n",
      "Getting Documents from JKS Fillings: 100%|██████████| 9/9 [00:00<00:00, 12.28filling/s]\n",
      "Getting Documents from JMU Fillings: 100%|██████████| 4/4 [00:00<00:00, 22.40filling/s]\n",
      "Getting Documents from JMEI Fillings: 100%|██████████| 5/5 [00:00<00:00, 19.43filling/s]\n",
      "Getting Documents from JP Fillings: 100%|██████████| 4/4 [00:00<00:00, 20.26filling/s]\n",
      "Getting Documents from KZ Fillings: 100%|██████████| 12/12 [00:00<00:00, 28.71filling/s]\n",
      "Getting Documents from LEJU Fillings: 100%|██████████| 5/5 [00:00<00:00, 22.89filling/s]\n",
      "Getting Documents from LX Fillings: 100%|██████████| 2/2 [00:00<00:00, 13.93filling/s]\n",
      "Getting Documents from LITB Fillings: 100%|██████████| 6/6 [00:00<00:00, 24.10filling/s]\n",
      "Getting Documents from MOMO Fillings: 100%|██████████| 5/5 [00:00<00:00, 21.02filling/s]\n",
      "Getting Documents from NTES Fillings: 100%|██████████| 18/18 [00:01<00:00, 13.63filling/s]\n",
      "Getting Documents from EDU Fillings: 100%|██████████| 12/12 [00:00<00:00, 16.78filling/s]\n",
      "Getting Documents from NIO Fillings: 100%|██████████| 1/1 [00:00<00:00,  6.00filling/s]\n",
      "Getting Documents from NOAH Fillings: 100%|██████████| 9/9 [00:00<00:00,  9.17filling/s]\n",
      "Getting Documents from ONE Fillings: 100%|██████████| 1/1 [00:00<00:00,  4.92filling/s]\n",
      "Getting Documents from OSN Fillings: 100%|██████████| 9/9 [00:01<00:00,  8.82filling/s]\n",
      "Getting Documents from PTR Fillings: 100%|██████████| 18/18 [00:00<00:00, 24.53filling/s]\n",
      "Getting Documents from FENG Fillings: 100%|██████████| 8/8 [00:01<00:00,  5.25filling/s]\n",
      "Getting Documents from PDD Fillings: 100%|██████████| 1/1 [00:00<00:00, 11.79filling/s]\n",
      "Getting Documents from PPDF Fillings: 100%|██████████| 2/2 [00:00<00:00,  8.80filling/s]\n",
      "Getting Documents from QD Fillings: 100%|██████████| 2/2 [00:00<00:00,  6.10filling/s]\n",
      "Getting Documents from SOL Fillings: 100%|██████████| 12/12 [00:00<00:00, 18.09filling/s]\n",
      "Getting Documents from RENN Fillings: 100%|██████████| 7/7 [00:00<00:00, 10.97filling/s]\n",
      "Getting Documents from REDU Fillings: 100%|██████████| 2/2 [00:00<00:00, 28.24filling/s]\n",
      "Getting Documents from RYB Fillings: 100%|██████████| 2/2 [00:00<00:00, 24.75filling/s]\n",
      "Getting Documents from SECO Fillings: 100%|██████████| 2/2 [00:00<00:00, 21.10filling/s]\n",
      "Getting Documents from SHI Fillings: 100%|██████████| 18/18 [00:00<00:00, 49.04filling/s]\n",
      "Getting Documents from SKYS Fillings: 100%|██████████| 4/4 [00:00<00:00, 24.76filling/s]\n",
      "Getting Documents from SOGO Fillings: 100%|██████████| 2/2 [00:00<00:00, 15.79filling/s]\n",
      "Getting Documents from SOHU Fillings: 100%|██████████| 1/1 [00:00<00:00,  6.27filling/s]\n",
      "Getting Documents from TAL Fillings: 100%|██████████| 8/8 [00:00<00:00, 10.06filling/s]\n",
      "Getting Documents from TEDU Fillings: 100%|██████████| 4/4 [00:00<00:00, 19.47filling/s]\n",
      "Getting Documents from TME Fillings: 100%|██████████| 1/1 [00:00<00:00,  9.93filling/s]\n",
      "Getting Documents from NCTY Fillings: 100%|██████████| 15/15 [00:00<00:00, 31.40filling/s]\n",
      "Getting Documents from TC Fillings: 100%|██████████| 1/1 [00:00<00:00, 23.87filling/s]\n",
      "Getting Documents from TOUR Fillings: 100%|██████████| 5/5 [00:00<00:00, 16.99filling/s]\n",
      "Getting Documents from VIPS Fillings: 100%|██████████| 7/7 [00:00<00:00,  9.20filling/s]\n",
      "Getting Documents from WB Fillings: 100%|██████████| 5/5 [00:01<00:00,  4.23filling/s]\n",
      "Getting Documents from XYF Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.17filling/s]\n",
      "Getting Documents from XIN Fillings: 100%|██████████| 12/12 [00:01<00:00,  6.95filling/s]\n",
      "Getting Documents from XNET Fillings: 100%|██████████| 5/5 [00:00<00:00, 12.20filling/s]\n",
      "Getting Documents from YIN Fillings: 100%|██████████| 3/3 [00:00<00:00, 11.57filling/s]\n",
      "Getting Documents from YRD Fillings: 100%|██████████| 4/4 [00:00<00:00, 10.39filling/s]\n",
      "Getting Documents from YY Fillings: 100%|██████████| 7/7 [00:01<00:00,  4.55filling/s]\n",
      "Getting Documents from ZLAB Fillings: 100%|██████████| 2/2 [00:00<00:00,  5.60filling/s]\n",
      "Getting Documents from ZPIN Fillings: 100%|██████████| 3/3 [00:00<00:00,  7.21filling/s]\n",
      "Getting Documents from ZTO Fillings: 100%|██████████| 3/3 [00:00<00:00, 16.80filling/s]\n",
      "Getting Documents from ZXAIY Fillings: 100%|██████████| 8/8 [00:00<00:00, 30.04filling/s]\n",
      "Getting Documents from LDKYQ Fillings: 100%|██████████| 8/8 [00:00<00:00, 26.47filling/s]\n",
      "Getting Documents from LASLY Fillings: 100%|██████████| 4/4 [00:00<00:00, 25.88filling/s]\n",
      "Getting Documents from LGFTY Fillings: 100%|██████████| 3/3 [00:00<00:00, 115.69filling/s]\n",
      "Getting Documents from YZCAY Fillings: 100%|██████████| 15/15 [00:00<00:00, 40.00filling/s]\n",
      "Getting Documents from ZAHLY Fillings: 100%|██████████| 5/5 [00:00<00:00, 98.30filling/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2019-04-26:\n",
      "\n",
      "<TYPE>20-F\n",
      "<SEQUENCE>1\n",
      "<FILENAME>d684592d20f.htm\n",
      "<DESCRIPTION>FORM 20-F\n",
      "<TEXT>\n",
      "<HTML><HEAD>\n",
      "<TITLE>Form 20-F</TITLE>\n",
      "</HEAD>\n",
      " <BODY BGCOLOR=\"WHITE\">\n",
      "<h5 align=\"left\"><a href=\"#toc\">Table of Contents<...\n",
      "\n",
      "Document 1 Filed on 2019-04-26:\n",
      "\n",
      "<TYPE>EX-8.1\n",
      "<SEQUENCE>2\n",
      "<FILENAME>d684592dex81.htm\n",
      "<DESCRIPTION>EX-8.1\n",
      "<TEXT>\n",
      "<HTML><HEAD>\n",
      "<TITLE>EX-8.1</TITLE>\n",
      "</HEAD>\n",
      " <BODY BGCOLOR=\"WHITE\">\n",
      "\n",
      "\n",
      "<Center><DIV STYLE=\"width:8.5in\" align=\"left\">\n",
      " <P ...\n",
      "\n",
      "Document 2 Filed on 2019-04-26:\n",
      "\n",
      "<TYPE>EX-12.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>d684592dex121.htm\n",
      "<DESCRIPTION>EX-12.1\n",
      "<TEXT>\n",
      "<HTML><HEAD>\n",
      "<TITLE>EX-12.1</TITLE>\n",
      "</HEAD>\n",
      " <BODY BGCOLOR=\"WHITE\">\n",
      "\n",
      "\n",
      "<Center><DIV STYLE=\"width:8.5in\" align=\"left\">\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wMm1S5vKz1i"
   },
   "source": [
    "### Get Document Types\n",
    "Now that we have all the documents, we want to find the 20-f form in this 20-f filing. Implement the `get_document_type` function to return the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`. Make sure to return the type as lowercase, so this example would be returned as \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcqKinFCKz1i"
   },
   "outputs": [],
   "source": [
    "def get_document_type(doc):\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    doc_type = [x[len('<TYPE>'):] for x in type_pattern.findall(doc)]\n",
    "    return doc_type[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "em0xx4_OKz1k"
   },
   "source": [
    "With the `get_document_type` function, we'll filter out all non 20-F documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LiTHkkLOKz1k"
   },
   "outputs": [],
   "source": [
    "twenty_Fs_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    twenty_Fs_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '20-f':\n",
    "                twenty_Fs_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '1506756'\n",
      "    file: '\\n<TYPE>20-F\\n<SEQUENCE>1\\n<FILENAME>d684592d20f....\n",
      "    file_date: '2019-04-26'},\n",
      "  {\n",
      "    cik: '1506756'\n",
      "    file: '\\n<TYPE>20-F\\n<SEQUENCE>1\\n<FILENAME>d511822d20f....\n",
      "    file_date: '2018-04-27'},\n",
      "  {\n",
      "    cik: '1506756'\n",
      "    file: '\\n<TYPE>20-F\\n<SEQUENCE>1\\n<FILENAME>d254317d20f....\n",
      "    file_date: '2017-04-28'},\n",
      "  {\n",
      "    cik: '1506756'\n",
      "    file: '\\n<TYPE>20-F\\n<SEQUENCE>1\\n<FILENAME>d156239d20f....\n",
      "    file_date: '2016-04-28'},\n",
      "  {\n",
      "    cik: '1506756'\n",
      "    file: '\\n<TYPE>20-F\\n<SEQUENCE>1\\n<FILENAME>d912903d20f....\n",
      "    file_date: '2015-04-30'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print_twenty_F_data(twenty_Fs_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTUyc2zVKz1m"
   },
   "source": [
    "## Preprocess the Data\n",
    "### Clean Up\n",
    "The text for the documents are very messy. To clean this up, we'll remove the html and lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "of7HBzbiKz1m"
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "of7HBzbiKz1m"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yi8eCF3XKz1o"
   },
   "source": [
    "Using the `clean_text` function, we'll clean up all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eem1SsBVKz1o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning YI 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.92s/20-F]\n",
      "Cleaning VNET 20-Fs: 100%|██████████| 8/8 [00:20<00:00,  2.54s/20-F]\n",
      "Cleaning QFIN 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.45s/20-F]\n",
      "Cleaning WBAI 20-Fs: 100%|██████████| 6/6 [00:07<00:00,  1.19s/20-F]\n",
      "Cleaning JOBS 20-Fs: 100%|██████████| 15/15 [00:20<00:00,  1.36s/20-F]\n",
      "Cleaning WUBA 20-Fs: 100%|██████████| 6/6 [00:04<00:00,  1.2620-F/s]\n",
      "Cleaning ATV 20-Fs: 100%|██████████| 12/12 [00:17<00:00,  1.86s/20-F]\n",
      "Cleaning AMCN 20-Fs: 100%|██████████| 12/12 [00:12<00:00,  1.24s/20-F]\n",
      "Cleaning BABA 20-Fs: 100%|██████████| 4/4 [00:18<00:00,  4.49s/20-F]\n",
      "Cleaning ACH 20-Fs: 100%|██████████| 20/20 [01:04<00:00,  3.21s/20-F]\n",
      "Cleaning ATAI 20-Fs: 100%|██████████| 12/12 [00:18<00:00,  1.63s/20-F]\n",
      "Cleaning JG 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.25s/20-F]\n",
      "Cleaning ATHM 20-Fs: 100%|██████████| 6/6 [00:08<00:00,  1.46s/20-F]\n",
      "Cleaning BIDU 20-Fs: 100%|██████████| 14/14 [00:23<00:00,  1.37s/20-F]\n",
      "Cleaning BZUN 20-Fs: 100%|██████████| 4/4 [00:03<00:00,  1.1020-F/s]\n",
      "Cleaning BILI 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.64s/20-F]\n",
      "Cleaning BEDU 20-Fs: 100%|██████████| 2/2 [00:03<00:00,  1.75s/20-F]\n",
      "Cleaning CANG 20-Fs: 100%|██████████| 1/1 [00:02<00:00,  2.09s/20-F]\n",
      "Cleaning CYOU 20-Fs: 100%|██████████| 10/10 [00:15<00:00,  1.38s/20-F]\n",
      "Cleaning CMCM 20-Fs: 100%|██████████| 5/5 [00:11<00:00,  2.34s/20-F]\n",
      "Cleaning DL 20-Fs: 100%|██████████| 11/11 [00:15<00:00,  1.34s/20-F]\n",
      "Cleaning CEA 20-Fs: 100%|██████████| 18/18 [00:44<00:00,  2.50s/20-F]\n",
      "Cleaning JRJC 20-Fs: 100%|██████████| 15/15 [00:21<00:00,  1.29s/20-F]\n",
      "Cleaning LFC 20-Fs: 100%|██████████| 16/16 [01:04<00:00,  3.52s/20-F]\n",
      "Cleaning HTHT 20-Fs: 100%|██████████| 9/9 [00:14<00:00,  1.73s/20-F]\n",
      "Cleaning CHL 20-Fs: 100%|██████████| 19/19 [00:32<00:00,  1.72s/20-F]\n",
      "Cleaning CEO 20-Fs: 100%|██████████| 18/18 [00:47<00:00,  2.94s/20-F]\n",
      "Cleaning BORN 20-Fs: 100%|██████████| 8/8 [00:08<00:00,  1.0820-F/s]\n",
      "Cleaning COE 20-Fs: 100%|██████████| 3/3 [00:04<00:00,  1.44s/20-F]\n",
      "Cleaning SNP 20-Fs: 100%|██████████| 18/18 [00:43<00:00,  2.44s/20-F]\n",
      "Cleaning XRF 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.10s/20-F]\n",
      "Cleaning ZNH 20-Fs: 100%|██████████| 18/18 [00:43<00:00,  2.42s/20-F]\n",
      "Cleaning CNTF 20-Fs: 100%|██████████| 13/13 [00:15<00:00,  1.11s/20-F]\n",
      "Cleaning CHA 20-Fs: 100%|██████████| 17/17 [00:35<00:00,  1.89s/20-F]\n",
      "Cleaning CHU 20-Fs: 100%|██████████| 18/18 [01:03<00:00,  3.53s/20-F]\n",
      "Cleaning CCIH 20-Fs: 100%|██████████| 8/8 [00:15<00:00,  1.91s/20-F]\n",
      "Cleaning CCM 20-Fs: 100%|██████████| 10/10 [00:16<00:00,  1.99s/20-F]\n",
      "Cleaning CTRP 20-Fs: 100%|██████████| 16/16 [00:23<00:00,  1.17s/20-F]\n",
      "Cleaning DQ 20-Fs: 100%|██████████| 9/9 [00:08<00:00,  1.18s/20-F]\n",
      "Cleaning EHIC 20-Fs: 100%|██████████| 4/4 [00:05<00:00,  1.30s/20-F]\n",
      "Cleaning SFUN 20-Fs: 100%|██████████| 8/8 [00:11<00:00,  1.57s/20-F]\n",
      "Cleaning FANH 20-Fs: 100%|██████████| 12/12 [00:19<00:00,  1.66s/20-F]\n",
      "Cleaning GDS 20-Fs: 100%|██████████| 3/3 [00:06<00:00,  2.05s/20-F]\n",
      "Cleaning GHG 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.46s/20-F]\n",
      "Cleaning GSUM 20-Fs: 100%|██████████| 3/3 [00:03<00:00,  1.25s/20-F]\n",
      "Cleaning GSH 20-Fs: 100%|██████████| 18/18 [00:26<00:00,  1.48s/20-F]\n",
      "Cleaning HLG 20-Fs: 100%|██████████| 4/4 [00:04<00:00,  1.02s/20-F]\n",
      "Cleaning HQCL 20-Fs: 100%|██████████| 12/12 [00:21<00:00,  2.04s/20-F]\n",
      "Cleaning HX 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.36s/20-F]\n",
      "Cleaning HMI 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.28s/20-F]\n",
      "Cleaning HNP 20-Fs: 100%|██████████| 18/18 [01:01<00:00,  3.39s/20-F]\n",
      "Cleaning HCM 20-Fs: 100%|██████████| 3/3 [00:15<00:00,  5.20s/20-F]\n",
      "Cleaning HUYA 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.65s/20-F]\n",
      "Cleaning KANG 20-Fs: 100%|██████████| 5/5 [00:12<00:00,  2.49s/20-F]\n",
      "Cleaning IQ 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.81s/20-F]\n",
      "Cleaning JD 20-Fs: 100%|██████████| 5/5 [00:10<00:00,  2.13s/20-F]\n",
      "Cleaning JT 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.27s/20-F]\n",
      "Cleaning JKS 20-Fs: 100%|██████████| 9/9 [00:10<00:00,  1.27s/20-F]\n",
      "Cleaning JMU 20-Fs: 100%|██████████| 4/4 [00:03<00:00,  1.0520-F/s]\n",
      "Cleaning JMEI 20-Fs: 100%|██████████| 5/5 [00:04<00:00,  1.1520-F/s]\n",
      "Cleaning JP 20-Fs: 100%|██████████| 4/4 [00:06<00:00,  1.62s/20-F]\n",
      "Cleaning KZ 20-Fs: 100%|██████████| 12/12 [00:16<00:00,  1.31s/20-F]\n",
      "Cleaning LEJU 20-Fs: 100%|██████████| 5/5 [00:06<00:00,  1.25s/20-F]\n",
      "Cleaning LX 20-Fs: 100%|██████████| 2/2 [00:04<00:00,  2.11s/20-F]\n",
      "Cleaning LITB 20-Fs: 100%|██████████| 6/6 [00:07<00:00,  1.24s/20-F]\n",
      "Cleaning MOMO 20-Fs: 100%|██████████| 5/5 [00:05<00:00,  1.05s/20-F]\n",
      "Cleaning NTES 20-Fs: 100%|██████████| 18/18 [00:24<00:00,  1.39s/20-F]\n",
      "Cleaning EDU 20-Fs: 100%|██████████| 12/12 [00:20<00:00,  1.48s/20-F]\n",
      "Cleaning NIO 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.01s/20-F]\n",
      "Cleaning NOAH 20-Fs: 100%|██████████| 9/9 [00:14<00:00,  1.32s/20-F]\n",
      "Cleaning ONE 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.74s/20-F]\n",
      "Cleaning OSN 20-Fs: 100%|██████████| 9/9 [00:06<00:00,  1.1920-F/s]\n",
      "Cleaning PTR 20-Fs: 100%|██████████| 18/18 [01:15<00:00,  6.99s/20-F]\n",
      "Cleaning FENG 20-Fs: 100%|██████████| 8/8 [00:15<00:00,  1.94s/20-F]\n",
      "Cleaning PDD 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.75s/20-F]\n",
      "Cleaning PPDF 20-Fs: 100%|██████████| 2/2 [00:03<00:00,  1.66s/20-F]\n",
      "Cleaning QD 20-Fs: 100%|██████████| 2/2 [00:03<00:00,  1.90s/20-F]\n",
      "Cleaning SOL 20-Fs: 100%|██████████| 12/12 [00:12<00:00,  1.30s/20-F]\n",
      "Cleaning RENN 20-Fs: 100%|██████████| 7/7 [00:15<00:00,  2.35s/20-F]\n",
      "Cleaning REDU 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.27s/20-F]\n",
      "Cleaning RYB 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.43s/20-F]\n",
      "Cleaning SECO 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.46s/20-F]\n",
      "Cleaning SHI 20-Fs: 100%|██████████| 18/18 [00:39<00:00,  2.17s/20-F]\n",
      "Cleaning SKYS 20-Fs: 100%|██████████| 4/4 [00:10<00:00,  2.74s/20-F]\n",
      "Cleaning SOGO 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.34s/20-F]\n",
      "Cleaning SOHU 20-Fs: 100%|██████████| 1/1 [00:02<00:00,  2.21s/20-F]\n",
      "Cleaning TAL 20-Fs: 100%|██████████| 8/8 [00:11<00:00,  1.46s/20-F]\n",
      "Cleaning TEDU 20-Fs: 100%|██████████| 4/4 [00:03<00:00,  1.0720-F/s]\n",
      "Cleaning TME 20-Fs: 100%|██████████| 1/1 [00:02<00:00,  2.10s/20-F]\n",
      "Cleaning NCTY 20-Fs: 100%|██████████| 15/15 [00:19<00:00,  1.40s/20-F]\n",
      "Cleaning TC 20-Fs: 100%|██████████| 1/1 [00:01<00:00,  1.03s/20-F]\n",
      "Cleaning TOUR 20-Fs: 100%|██████████| 5/5 [00:05<00:00,  1.16s/20-F]\n",
      "Cleaning VIPS 20-Fs: 100%|██████████| 7/7 [00:14<00:00,  1.83s/20-F]\n",
      "Cleaning WB 20-Fs: 100%|██████████| 5/5 [00:07<00:00,  1.50s/20-F]\n",
      "Cleaning XYF 20-Fs: 100%|██████████| 1/1 [00:02<00:00,  2.17s/20-F]\n",
      "Cleaning XIN 20-Fs: 100%|██████████| 12/12 [00:23<00:00,  1.93s/20-F]\n",
      "Cleaning XNET 20-Fs: 100%|██████████| 5/5 [00:06<00:00,  1.38s/20-F]\n",
      "Cleaning YIN 20-Fs: 100%|██████████| 3/3 [00:06<00:00,  2.03s/20-F]\n",
      "Cleaning YRD 20-Fs: 100%|██████████| 4/4 [00:07<00:00,  1.70s/20-F]\n",
      "Cleaning YY 20-Fs: 100%|██████████| 7/7 [00:11<00:00,  1.83s/20-F]\n",
      "Cleaning ZLAB 20-Fs: 100%|██████████| 2/2 [00:02<00:00,  1.07s/20-F]\n",
      "Cleaning ZPIN 20-Fs: 100%|██████████| 3/3 [00:03<00:00,  1.0120-F/s]\n",
      "Cleaning ZTO 20-Fs: 100%|██████████| 3/3 [00:04<00:00,  1.48s/20-F]\n",
      "Cleaning ZXAIY 20-Fs: 100%|██████████| 8/8 [00:14<00:00,  1.88s/20-F]\n",
      "Cleaning LDKYQ 20-Fs: 100%|██████████| 8/8 [00:18<00:00,  2.19s/20-F]\n",
      "Cleaning LASLY 20-Fs: 100%|██████████| 4/4 [00:09<00:00,  2.39s/20-F]\n",
      "Cleaning LGFTY 20-Fs: 100%|██████████| 3/3 [00:06<00:00,  2.18s/20-F]\n",
      "Cleaning YZCAY 20-Fs: 100%|██████████| 15/15 [00:43<00:00,  2.93s/20-F]\n",
      "Cleaning ZAHLY 20-Fs: 100%|██████████| 5/5 [00:06<00:00,  1.33s/20-F]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_clean: '\\n20-f\\n1\\nd684592d20f.htm\\nform 20-f\\n\\n\\nform 2...},\n",
      "  {\n",
      "    file_clean: '\\n20-f\\n1\\nd511822d20f.htm\\nform 20-f\\n\\n\\nform 2...},\n",
      "  {\n",
      "    file_clean: '\\n20-f\\n1\\nd254317d20f.htm\\nform 20-f\\n\\n\\nform 2...},\n",
      "  {\n",
      "    file_clean: '\\n20-f\\n1\\nd156239d20f.htm\\nform 20-f\\n\\n\\nform 2...},\n",
      "  {\n",
      "    file_clean: '\\n20-f\\n1\\nd912903d20f.htm\\nform 20-f\\n\\n\\nform 2...},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# parallelize \n",
    "\n",
    "for ticker, twenty_Fs in twenty_Fs_by_ticker.items():\n",
    "    for twenty_F in tqdm(twenty_Fs, desc='Cleaning {} 20-Fs'.format(ticker), unit='20-F'):\n",
    "        twenty_F['file_clean'] = clean_text(twenty_F['file'])\n",
    "\n",
    "print_twenty_F_data(twenty_Fs_by_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tO0DmCkcKz1q"
   },
   "source": [
    "### Lemmatize\n",
    "With the text cleaned up, it's time to distill the verbs down. Implement the `lemmatize_words` function to lemmatize verbs in the list of words provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KV3S5_IQKz1r"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrsURjk4Kz1t"
   },
   "source": [
    "With the `lemmatize_words` function implemented, let's lemmatize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_Fs_by_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[k,v] for k,v in twenty_Fs_by_ticker.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def lemmatize_words(words):\n",
    "#     lemmatized_words =    \n",
    "#     return lemmatized_words\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def prange_test():\n",
    "    s = 0\n",
    "    # Without \"parallel=True\" in the jit-decorator\n",
    "    # the prange statement is equivalent to range\n",
    "    \n",
    "    word_pattern = re.compile('\\w+')\n",
    "\n",
    "\n",
    "    for i in prange(len(l)):\n",
    "        ticker,twenty_Fs = l[i]\n",
    "#         k = [[k,v] for k,v in twenty_Fs.items()]\n",
    "        for j in prange(len(twenty_Fs)):\n",
    "#         for twenty_F in tqdm(twenty_Fs, desc='Lemmatize {} 20-Fs'.format(ticker), unit='20-F'):\n",
    "            twenty_Fs[j]['file_lemma'] = [WordNetLemmatizer().lemmatize(w, pos='v') for w in word_pattern.findall(twenty_Fs[j]['file_clean'])]\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "nlp = Language(Vocab())\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-c9a3152c632d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-c9a3152c632d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tokenizer.\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmas = lemmatizer(u\"ducks\", u\"NOUN\")\n",
    "assert lemmas == [u\"duck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prange_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQBRqLBQKz1u"
   },
   "outputs": [],
   "source": [
    "# para it.....\n",
    "\n",
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, twenty_Fs in twenty_Fs_by_ticker.items():\n",
    "    for twenty_F in tqdm(twenty_Fs, desc='Lemmatize {} 20-Fs'.format(ticker), unit='20-F'):\n",
    "        twenty_F['file_lemma'] = lemmatize_words(word_pattern.findall(twenty_F['file_clean']))\n",
    "\n",
    "print_twenty_F_data(twenty_Fs_by_ticker[example_ticker][:5], ['file_lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ez4gpD_QKz1x"
   },
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFpV2ygvKz1y"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, twenty_Fs in twenty_Fs_by_ticker.items():\n",
    "    for twenty_F in tqdm(twenty_Fs, desc='Remove Stop Words for {} 20-Fs'.format(ticker), unit='20-F'):\n",
    "        twenty_F['file_lemma'] = [word for word in twenty_F['file_lemma'] if word not in lemma_english_stopwords]\n",
    "\n",
    "\n",
    "print('Stop Words Removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wdw73iOoKz1z"
   },
   "source": [
    "## Analysis on 20fs\n",
    "### Loughran McDonald Sentiment Word Lists\n",
    "We'll be using the Loughran and McDonald sentiment word lists. These word lists cover the following sentiment:\n",
    "- Negative \n",
    "- Positive\n",
    "- Uncertainty\n",
    "- Litigious\n",
    "- Constraining\n",
    "- Superfluous\n",
    "- Modal\n",
    "\n",
    "This will allow us to do the sentiment analysis on the 20-Fs. Let's first load these word lists. We'll be looking into a few of these sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJqY9_SOK7vJ"
   },
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('LoughranMcDonald_MasterDictionary_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AJurs-_Kz10"
   },
   "outputs": [],
   "source": [
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns] # Lowercase the columns for ease of use\n",
    "\n",
    "# Remove unused information\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# Apply the same preprocessing to these words as the 20-F words\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv('sentimentDataFrame.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDQk7kcAKz12"
   },
   "source": [
    "### Bag of Words\n",
    "using the sentiment word lists, let's generate sentiment bag of words from the 20-F documents. Implement `get_bag_of_words` to generate a bag of words that counts the number of sentiment words in each doc. You can ignore words that are not in `sentiment_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZAjs17XKz12"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(sentiment_words, docs):\n",
    "    cnt = Counter()\n",
    "    \n",
    "    cv = CountVectorizer(vocabulary=sentiment_words)\n",
    "    bag_of_words =  cv.fit_transform(docs).toarray()\n",
    "    \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoE1dx0JKz15"
   },
   "source": [
    "Using the `get_bag_of_words` function, we'll generate a bag of words for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XR5LaYOJKz15"
   },
   "outputs": [],
   "source": [
    "sentiment_bow_twenty_Fs = {}\n",
    "\n",
    "for ticker, twenty_Fs in twenty_Fs_by_ticker.items():\n",
    "    lemma_docs = [' '.join(twenty_F['file_lemma']) for twenty_F in twenty_Fs]\n",
    "    \n",
    "    sentiment_bow_twenty_Fs[ticker] = {\n",
    "        sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "\n",
    "print_twenty_F_data([sentiment_bow_twenty_Fs[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmEuJz7lKz16"
   },
   "source": [
    "### Jaccard Similarity\n",
    "Using the bag of words, let's calculate the jaccard similarity on the bag of words and plot it over time. Implement `get_jaccard_similarity` to return the jaccard similarities between each tick in time. Since the input, `bag_of_words_matrix`, is a bag of words for each time period in order, you just need to compute the jaccard similarities for each neighboring bag of words. Make sure to turn the bag of words into a boolean array when calculating the jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXAuhx2fKz19"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "\n",
    "def get_jaccard_similarity(bag_of_words_matrix):\n",
    "    bool_array = np.array(bag_of_words_matrix, dtype=bool)\n",
    "    jaccard_similarities = [jaccard_similarity_score(bool_array[i],bool_array[i+1]) \\\n",
    "                           for i in range(len(bool_array)-1)]\n",
    "    return jaccard_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40ezQ0TAKz1-"
   },
   "source": [
    "Using the `get_jaccard_similarity` function, let's plot the similarities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gZ4xI_rKz1-"
   },
   "outputs": [],
   "source": [
    "# Get dates for the universe\n",
    "file_dates = {\n",
    "    ticker: [twenty_F['file_date'] for twenty_F in twenty_Fs]\n",
    "    for ticker, twenty_Fs in twenty_Fs_by_ticker.items()}  \n",
    "\n",
    "jaccard_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_jaccard_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in twenty_F_sentiments.items()}\n",
    "    for ticker, twenty_F_sentiments in sentiment_bow_twenty_Fs.items()}\n",
    "\n",
    "\n",
    "plot_similarities(\n",
    "    [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Jaccard Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAyPi0buKz2A"
   },
   "source": [
    "### TFIDF\n",
    "using the sentiment word lists, hereby generate sentiment TFIDF from the 20-f documents, i.e. the more the word appears, the more weighting is given to the sentiment associated with the word. Here `get_tfidf` generates TFIDF from each document, using sentiment words as the terms, words that are not in `sentiment_words` were ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_ViGl6fKz2A"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def get_tfidf(sentiment_words, docs):\n",
    "    tf_idfVector = TfidfVectorizer(vocabulary = sentiment_words)\n",
    "    tfidf = tf_idfVector.fit_transform(docs).toarray()\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPYKjaH_Kz2C"
   },
   "source": [
    "Using the `get_tfidf` function, let's generate the TFIDF values for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df[sentiment]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrWlB_vAKz2D"
   },
   "outputs": [],
   "source": [
    "sentiment_tfidf_twenty_Fs = {}\n",
    "\n",
    "for ticker, twenty_Fs in twenty_Fs_by_ticker.items():\n",
    "    lemma_docs = [' '.join(twenty_F['file_lemma']) for twenty_F in twenty_Fs]\n",
    "    \n",
    "    sentiment_tfidf_twenty_Fs[ticker] = { sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs) for sentiment in sentiments}\n",
    "\n",
    "    \n",
    "print_twenty_F_data([sentiment_tfidf_twenty_Fs[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXc-TJQqKz2E"
   },
   "source": [
    "### Cosine Similarity\n",
    "Using the TFIDF values, the cosine similarity over time was calculated and plotted. Here `get_cosine_similarity` was used to return the cosine similarities between each tick in time. The input `tfidf_matrix` is a TFIDF vector for each time period in order, thus only the cosine similarities for each neighboring vector need be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tfidf_twenty_Fs['YI']['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1e9NgX-Kz2F"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "\n",
    "\n",
    "# cosine_similarities(sentiments)\n",
    "\n",
    "def get_cosine_similarity(sentiment_tfidf_twenty_Fs):\n",
    "\n",
    "    sentiments_cosine_similarity ={}\n",
    "    \n",
    "    for i,j in combinations(sentiment_tfidf_twenty_Fs.keys(),2):\n",
    "        u = sentiment_tfidf_twenty_Fs[i]\n",
    "        v = sentiment_tfidf_twenty_Fs[j]\n",
    "        for sentiment in sentiments:\n",
    "#             print(i,j,sentiment,cosine_similarity(u[sentiment],v[sentiment]))\n",
    "            u = tfidf_matrix[i].reshape(1,-1)\n",
    "            v = tfidf_matrix[i+1].reshape(1,-1)\n",
    "            cosine_similarities.append(cosine_similarity(u,v)[0][0])\n",
    "\n",
    "            break\n",
    "# def get_cosine_similarity(tfidf_matrix):\n",
    "# # Get cosine similarities for each neighboring TFIDF vector/document\n",
    "# # tfidf : 2-d Numpy Ndarray of floatTFIDF sentiment for each document, the first dimension is the document, the second dimension is the word\n",
    "#     cosine_similarities = []\n",
    "#     for i in range(len(tfidf_matrix)-1):\n",
    "#         u = tfidf_matrix[i].reshape(1,-1)\n",
    "#         v = tfidf_matrix[i+1].reshape(1,-1)\n",
    "#         cosine_similarities.append(cosine_similarity(u,v)[0][0])\n",
    "#     return cosine_similarities\n",
    "\n",
    "# Returns cosine_similarities: list of float\n",
    "# Cosine similarities for neighboring documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tfidf_twenty_Fs['YI']['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tfidf_twenty_Fs['YI']['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " get_cosine_similarity(sentiment_tfidf_twenty_Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtZ5TWGFKz2H"
   },
   "source": [
    "Let's plot the cosine similarities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdpkufooKz2H"
   },
   "outputs": [],
   "source": [
    "cosine_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_cosine_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in twenty_F_sentiments.items()}\n",
    "    for ticker, twenty_F_sentiments in sentiment_tfidf_twenty_Fs.items()}\n",
    "\n",
    "\n",
    "plot_similarities(\n",
    "    [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Cosine Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save outputs as text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarities_df = pd.DataFrame.from_dict(jaccard_similarities)\n",
    "jaccard_similarities_df.to_json('jaccard_similarities.json')\n",
    "\n",
    "sentiment_tfidf_twenty_Fs_df = pd.DataFrame.from_dict(jaccard_similarities)\n",
    "sentiment_tfidf_twenty_Fs_df.to_jason('sentiment_tfidf_twenty_Fs.json')\n",
    "\n",
    "cosine_similarities_df = pd.DataFrame.from_dict(cosine_similarities)\n",
    "cosine_similarities_df.to_jason('cosine_similarities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cosine_similarities.json', 'w') as file:\n",
    "    file.write(json.dumps(cosine_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filling_documents_by_ticker.json', 'w') as file:\n",
    "    file.write(json.dumps(filling_documents_by_ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file_dates.json', 'w') as file:\n",
    "    file.write(json.dumps(file_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twenty_Fs_by_ticker.json', 'w') as file:\n",
    "    file.write(json.dumps(twenty_Fs_by_ticker))    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Gj1B9gt-Kz2N",
    "WhIHd16AKz2O"
   ],
   "name": "CharisFinancialStatementNLP.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
